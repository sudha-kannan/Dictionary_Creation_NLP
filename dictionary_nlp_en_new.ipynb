{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, csv, collections, itertools\n",
    "import numpy as np\n",
    "import stanfordnlp\n",
    "from polyglot.text import Text, Word\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "config = {\n",
    "    'use_gpu' : True,\n",
    "    'processors': 'tokenize,pos,lemma',  # Comma-separated list of processors to use\n",
    "    'lang': 'en',  # Language code for the language to build the Pipeline in\n",
    "    'tokenize_model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt',\n",
    "    # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "    'pos_model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt',\n",
    "    'pos_pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt',\n",
    "    'lemma_model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " n\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "stanfordnlp.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0M'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0M'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(inputf, wiegand_dict):\n",
    "    t_l = []\n",
    "    p_l = []\n",
    "    offense = pd.read_csv(inputf, delimiter=\"\\t\", header=0, usecols=['id', 'tweet', 'subtask_a'])\n",
    "    clean_na = offense.dropna()\n",
    "\n",
    "    msg_tokenized_stemmed = tokenize_and_stem_msg(clean_na)\n",
    "    wiegand_en = stem_wiegand(wiegand_dict)\n",
    "\n",
    " \n",
    "    pred_v5, pred_tok = prediction_v5(msg_tokenized_stemmed,wiegand_en)\n",
    "    \n",
    "    \n",
    "    with open(\"/root/standfordnlp/offenseval-training-v2.tsv\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(\"\\t\")\n",
    "            if not str(values[2]).startswith('subtask_a'):\n",
    "                  True_Label.append(values[2])\n",
    "    \n",
    "    \n",
    "    t_l = np.array([[x] for x in True_Label])\n",
    "    values_5 = list(pred_v5.values())\n",
    "    #Predicted_Label = value_5\n",
    "    p_l = np.array([[x] for x in values_5]) \n",
    "    print(p_l.shape)\n",
    "    f = f1_score(t_l, p_l, average='macro')\n",
    "    print(\"F1 Score: \")\n",
    "    print(f)\n",
    "    #values_tok_6 = list(pred_tok.values())\n",
    "    #values_tok_l = list(pred_l.values())\n",
    "    pred_df_5 = pd.DataFrame({'pred_labels': values_5})\n",
    "    pred_df = pd.DataFrame(pred_tok)\n",
    "    final_format_v5 = offense.join(pred_df_5, how='outer')\n",
    "\n",
    "    outfile5 = \"hurt_key_extra_en_8.tsv\"\n",
    "    output = \"tokenized_en_8.tsv\"\n",
    "    pred_df.to_csv(output, sep = \"\\t\", index = False)\n",
    "    final_format_v5.to_csv(outfile5, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem_msg(offense_df):\n",
    "\n",
    "    msg = offense_df['tweet']\n",
    "    pre_proc_data = collections.defaultdict(list)\n",
    "    nlp = stanfordnlp.Pipeline(**config)\n",
    "    for idx, elem in enumerate(msg):\n",
    "        doc = nlp(elem)  # Run the pipeline on input text\n",
    "        token_text = [word.text for sent in doc.sentences for word in sent.words]\n",
    "        token_lemma = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "        token_pos = [word.upos for sent in doc.sentences for word in sent.words]\n",
    "        final = list(zip(token_lemma, token_pos, token_text))\n",
    "        \n",
    "\n",
    "        pre_proc_data[idx] = final\n",
    "\n",
    "        \n",
    "   \n",
    "    print(\"Done lemmatization...\")\n",
    "    return pre_proc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_wiegand(keyf):\n",
    "\n",
    "    stemmed_swear_en = []\n",
    "\n",
    "    with open(keyf) as f:\n",
    "        for line in f:\n",
    "            line_splitted = line.strip().split(\"\\t\")\n",
    "\n",
    "            if float(line_splitted[1]) >= 0.75:\n",
    "                target_tok = line_splitted[0].split(\"_\")[0]\n",
    "                if target_tok not in stemmed_swear_en:\n",
    "                    stemmed_swear_en.append(target_tok)\n",
    "\n",
    "    return stemmed_swear_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_v5(tweet_tok_pos_stem, wiegand_en):\n",
    "    \n",
    "    \n",
    "    seen = set()\n",
    "    predicted_dict = {}\n",
    "    predicted_tok = []\n",
    "    #predicted_tok_l = {}\n",
    "    for k, v in tweet_tok_pos_stem.items():\n",
    "        for elem in v:\n",
    "            #print(\"this is elem   \")\n",
    "            #print(elem)\n",
    "            tok, pos, tok_word = elem\n",
    "            #print(\"this is tok, pos , tok_l\" +tok + \" \"+ pos +\" \"+ tok_word)\n",
    "            if tok in wiegand_en:\n",
    "                predicted_dict[k] = \"OFF\"\n",
    "                predicted_tok.append(str(tok_word)+ \"\\t\"+ \"OFF\")\n",
    "                #predicted_tok[k] = tok_word\n",
    "                #print(\"this is offense \" + tok+ \" \" + tok_word)\n",
    "            #else:\n",
    "             #   predicted_tok[k] = tok_word\n",
    "              #  predicted_tok_l[k] = \"NOT\"\n",
    "                #print(tok)\n",
    "               # break\n",
    "                \n",
    "    for k, v in tweet_tok_pos_stem.items():\n",
    "            if k not in predicted_dict:\n",
    "                predicted_dict[k] = \"NOT\"\n",
    "                #predicted_tok_l[k] = \"NOT\"\n",
    "                #predicted_tok[k] = \n",
    "            #if k not in predicted_tok_l:\n",
    "             #   predicted_tok_l[k] = \"NOT\"\n",
    "                \n",
    "    for k, v in tweet_tok_pos_stem.items():\n",
    "            for elem in v:\n",
    "                tok, pos, tok_word = elem\n",
    "                if tok_word not in predicted_tok:\n",
    "                    if tok_word not in seen:\n",
    "                        seen.add(tok_word)\n",
    "                        predicted_tok.append(str(tok_word)+ \"\\t\"+ \"NOT\")\n",
    "                \n",
    "    \n",
    "    \n",
    "    return predicted_dict, predicted_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n",
      "Done lemmatization...\n",
      "(13240, 1)\n",
      "F1 Score: \n",
      "0.49179346905290283\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \"\"\"\n",
    "    activate miniconda: source./miniconda/bin/activate ; conda init\n",
    "    activate venv : conda activate offenseval_stanford\n",
    "    run stanford nlp for EN\n",
    "    \"\"\"\n",
    "    True_Label = []\n",
    "    Predicted_Label = []\n",
    "    training = \"/root/standfordnlp/offenseval-training-v2.tsv\"   #available_sudha\n",
    "    wiegand_dict = \"/root/standfordnlp/wiegand_expanded\"\n",
    "    read_data(training, wiegand_dict)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
